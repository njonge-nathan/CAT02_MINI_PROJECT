{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOaiZ2NZW+1chz1StfftqWH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/njonge-nathan/CAT02_MINI_PROJECT/blob/main/CAT02_MINI_PROJECT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Student Number 094230\n",
        "\n",
        "CAT 2 - MINI PROJECT\n",
        "\n",
        "COMPILER CONSTRUCTION\n",
        "\n"
      ],
      "metadata": {
        "id": "JvGG7YjjDiWh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question"
      ],
      "metadata": {
        "id": "-QsjoZ8zD8j3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " CAT 2 is a mini project. Implement any phase, component, element etc learned in the course thus far.\n",
        "\n",
        "Ideas can range from implementing a lexer, parser to generating your own language (i.e. CFG) etc.\n",
        "\n",
        "Whatever you implement make sure to describe your solution in detail by outlining its logic, concepts used among other relevant details.\n",
        "\n",
        "Marks will be awarded based on:\n",
        "\n",
        "Uniqueness of idea (3 marks)\n",
        "\n",
        "Description of details and logic used (6 marks)\n",
        "\n",
        "A show of connection between the solution and the concept(s) learned in class (4 marks)\n",
        "\n",
        "Proper formatting and editing of the implementation notebook (2 marks)\n",
        "Total = 15 marks."
      ],
      "metadata": {
        "id": "CGMN5iY9Dxkq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution Implementation"
      ],
      "metadata": {
        "id": "Z3cbXPVmEBgY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The solution involves an implementation of a simple compiler construction solution that includes both lexical analysis (using a lexer) and syntactic analysis (using a parser) for a basic arithmetic expression language. We'll use Python for this example."
      ],
      "metadata": {
        "id": "OT2njhpmEPQy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lexer Implementation"
      ],
      "metadata": {
        "id": "e7CIu6ZOEbJz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cRll2gulDXZk"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "class Lexer:\n",
        "    \"\"\"\n",
        "    Lexer class for tokenizing arithmetic expressions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, expression):\n",
        "        \"\"\"\n",
        "        Initialize the Lexer with the input expression.\n",
        "\n",
        "        Parameters:\n",
        "        - expression (str): The arithmetic expression to tokenize.\n",
        "        \"\"\"\n",
        "        self.expression = expression\n",
        "        self.tokens = []\n",
        "        self.current_position = 0\n",
        "\n",
        "    def tokenize(self):\n",
        "        \"\"\"\n",
        "        Tokenize the input expression.\n",
        "\n",
        "        Returns:\n",
        "        - list: List of tokens representing the arithmetic expression.\n",
        "        \"\"\"\n",
        "        while self.current_position < len(self.expression):\n",
        "            char = self.expression[self.current_position]\n",
        "\n",
        "            if char.isspace():\n",
        "                # Skip whitespace\n",
        "                self.current_position += 1\n",
        "                continue\n",
        "            elif char.isdigit():\n",
        "                # Tokenize numbers\n",
        "                self.tokens.append(self.tokenize_number())\n",
        "            elif char in {'+', '-', '*', '/'}:\n",
        "                # Tokenize operators\n",
        "                self.tokens.append({'type': 'OPERATOR', 'value': char})\n",
        "                self.current_position += 1\n",
        "            elif char == '(' or char == ')':\n",
        "                # Tokenize parentheses\n",
        "                self.tokens.append({'type': 'PARENTHESIS', 'value': char})\n",
        "                self.current_position += 1\n",
        "            else:\n",
        "                raise ValueError(f\"Invalid character: {char}\")\n",
        "\n",
        "        return self.tokens\n",
        "\n",
        "    def tokenize_number(self):\n",
        "        \"\"\"\n",
        "        Tokenize a sequence of digits as a number.\n",
        "\n",
        "        Returns:\n",
        "        - dict: Token representing a number.\n",
        "        \"\"\"\n",
        "        number = ''\n",
        "        while self.current_position < len(self.expression) and self.expression[self.current_position].isdigit():\n",
        "            number += self.expression[self.current_position]\n",
        "            self.current_position += 1\n",
        "\n",
        "        return {'type': 'NUMBER', 'value': int(number)}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Description"
      ],
      "metadata": {
        "id": "kl7_qmlQFAlI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lexer: Tokenizes the input expression into a sequence of tokens. The Lexer class uses regular expressions to recognize numbers and operators."
      ],
      "metadata": {
        "id": "Ogu302pzEx6q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parser Implementation"
      ],
      "metadata": {
        "id": "vNjLxEylEk53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Parser:\n",
        "    \"\"\"\n",
        "    Parser class for parsing arithmetic expressions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tokens):\n",
        "        \"\"\"\n",
        "        Initialize the Parser with a list of tokens.\n",
        "\n",
        "        Parameters:\n",
        "        - tokens (list): List of tokens representing the arithmetic expression.\n",
        "        \"\"\"\n",
        "        self.tokens = tokens\n",
        "        self.current_position = 0\n",
        "\n",
        "    def parse(self):\n",
        "        \"\"\"\n",
        "        Parse the arithmetic expression.\n",
        "\n",
        "        Returns:\n",
        "        - dict: Abstract syntax tree (AST) representing the expression structure.\n",
        "        \"\"\"\n",
        "        return self.parse_expression()\n",
        "\n",
        "    def parse_expression(self):\n",
        "        \"\"\"\n",
        "        Parse terms and handle addition/subtraction.\n",
        "\n",
        "        Returns:\n",
        "        - dict: AST node representing the expression.\n",
        "        \"\"\"\n",
        "        left_operand = self.parse_term()\n",
        "\n",
        "        while self.current_position < len(self.tokens) and self.tokens[self.current_position]['type'] == 'OPERATOR' and self.tokens[self.current_position]['value'] in {'+', '-'}:\n",
        "            operator = self.tokens[self.current_position]['value']\n",
        "            self.current_position += 1\n",
        "            right_operand = self.parse_term()\n",
        "\n",
        "            left_operand = {'type': 'EXPRESSION', 'operator': operator, 'left': left_operand, 'right': right_operand}\n",
        "\n",
        "        return left_operand\n",
        "\n",
        "    def parse_term(self):\n",
        "        \"\"\"\n",
        "        Parse factors and handle multiplication/division.\n",
        "\n",
        "        Returns:\n",
        "        - dict: AST node representing the term.\n",
        "        \"\"\"\n",
        "        factor = self.parse_factor()\n",
        "\n",
        "        while self.current_position < len(self.tokens) and self.tokens[self.current_position]['type'] == 'OPERATOR' and self.tokens[self.current_position]['value'] in {'*', '/'}:\n",
        "            operator = self.tokens[self.current_position]['value']\n",
        "            self.current_position += 1\n",
        "            term = self.parse_factor()\n",
        "\n",
        "            factor = {'type': 'TERM', 'operator': operator, 'left': factor, 'right': term}\n",
        "\n",
        "        return factor\n",
        "\n",
        "    def parse_factor(self):\n",
        "        \"\"\"\n",
        "        Parse factors including numbers and expressions in parentheses.\n",
        "\n",
        "        Returns:\n",
        "        - dict: AST node representing the factor.\n",
        "        \"\"\"\n",
        "        if self.current_position < len(self.tokens):\n",
        "            current_token = self.tokens[self.current_position]\n",
        "\n",
        "            if current_token['type'] == 'NUMBER':\n",
        "                # If it's a number, consume the token and return it\n",
        "                self.current_position += 1\n",
        "                return current_token\n",
        "            elif current_token['type'] == 'PARENTHESIS' and current_token['value'] == '(':\n",
        "                # If it's an opening parenthesis, consume it, parse the expression inside, and check for a closing parenthesis\n",
        "                self.current_position += 1\n",
        "                expression = self.parse_expression()\n",
        "                if self.current_position < len(self.tokens) and self.tokens[self.current_position]['type'] == 'PARENTHESIS' and self.tokens[self.current_position]['value'] == ')':\n",
        "                    self.current_position += 1\n",
        "                    return expression\n",
        "\n",
        "        raise ValueError(\"Invalid expression syntax\")\n"
      ],
      "metadata": {
        "id": "bcWb2A0XEh_Z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Description"
      ],
      "metadata": {
        "id": "qjnYXWAMFEFB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parser: Parses the sequence of tokens into an abstract syntax tree (AST) that represents the structure of the expression.\n",
        "\n",
        "Parser Methods: The parser has methods for parsing expressions, terms, and factors. It follows the grammar rules for arithmetic expressions, considering operator precedence and parentheses.\n",
        "\n",
        "AST Representation: The AST is represented using nested dictionaries, where each node has a type ('EXPRESSION', 'TERM', 'NUMBER') and relevant values."
      ],
      "metadata": {
        "id": "7Lgn5ucAE81u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The provided Python code for the lexer and parser is designed to tokenize and parse arithmetic expressions, building an Abstract Syntax Tree (AST) to represent the structure of the expression."
      ],
      "metadata": {
        "id": "3ayEw3NMGfV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_ast(node, indent=\"\"):\n",
        "    if 'value' in node:\n",
        "        print(f\"{indent}Type: {node['type']}, Value: {node['value']}\")\n",
        "    else:\n",
        "        print(f\"{indent}Type: {node['type']}\")\n",
        "        print(f\"{indent}Operator: {node['operator']}\")\n",
        "        print_ast(node['left'], indent + \"  \")\n",
        "        print_ast(node['right'], indent + \"  \")\n",
        "\n",
        "# Example Usage\n",
        "expression = \"3 + 4 * (2 - 7) / 3\"\n",
        "lexer = Lexer(expression)\n",
        "tokens = lexer.tokenize()\n",
        "\n",
        "parser = Parser(tokens)\n",
        "parsed_tree = parser.parse()\n",
        "\n",
        "print_ast(parsed_tree)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZjYGcMEGnFL",
        "outputId": "ce6a5ee1-99f4-4c0c-f0b1-0205e2ab2d3d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type: EXPRESSION\n",
            "Operator: +\n",
            "  Type: NUMBER, Value: 3\n",
            "  Type: TERM\n",
            "  Operator: /\n",
            "    Type: TERM\n",
            "    Operator: *\n",
            "      Type: NUMBER, Value: 4\n",
            "      Type: EXPRESSION\n",
            "      Operator: -\n",
            "        Type: NUMBER, Value: 2\n",
            "        Type: NUMBER, Value: 7\n",
            "    Type: NUMBER, Value: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This print_ast function recursively traverses the AST and prints information about each node, indicating the type and value for leaf nodes (numbers) and the type and operator for internal nodes.\n",
        "\n",
        "For the provided example expression (\"3 + 4 * (2 - 7) / 3\"), the output of the print_ast"
      ],
      "metadata": {
        "id": "EFAuXuqwGzte"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This implementation connects to the concepts of compiler construction, specifically lexical and syntactic analysis.\n",
        "\n",
        "Lexer (Lexical Analysis): Tokenization is a crucial step in the lexical analysis phase. The lexer recognizes and categorizes characters into meaningful tokens.\n",
        "\n",
        "Parser (Syntactic Analysis): The parser processes the tokens and builds a hierarchical representation (AST) that reflects the syntactic structure of the input expression."
      ],
      "metadata": {
        "id": "3asUJIlBHIdC"
      }
    }
  ]
}